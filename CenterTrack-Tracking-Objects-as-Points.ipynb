{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CenterTrack-Tracking-Objects-as-Points.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNUNfnmzNfn/N7bOV4Ltcx9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hardik0/Multi-Object-Tracking-Google-Colab/blob/main/CenterTrack-Tracking-Objects-as-Points.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPw8WGdCfyk8"
      },
      "source": [
        "# CenterTrack - Tracking Objects as Points\r\n",
        "Simultaneous object detection and tracking using center points:\r\n",
        "<img src=\"https://github.com/xingyizhou/CenterTrack/raw/master/readme/fig2.png\">\r\n",
        "\r\n",
        "**Original code :** [CenterTrack](https://github.com/xingyizhou/CenterTrack) by [Xingyi Zhou](https://github.com/xingyizhou)\r\n",
        "\r\n",
        "**[Tracking Objects as Points](http://arxiv.org/abs/2004.01177)**,\r\n",
        "\r\n",
        "Xingyi Zhou, Vladlen Koltun, Philipp Krähenbühl,\r\n",
        "\r\n",
        "*arXiv technical report [(arXiv 2004.01177)](http://arxiv.org/abs/2004.01177)*\r\n",
        "\r\n",
        "**Citation**\r\n",
        "```\r\n",
        "@article{zhou2020tracking,\r\n",
        "  title={Tracking Objects as Points},\r\n",
        "  author={Zhou, Xingyi and Koltun, Vladlen and Kr{\\\"a}henb{\\\"u}hl, Philipp},\r\n",
        "  journal={ECCV},\r\n",
        "  year={2020}\r\n",
        "}\r\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMWkS8jt0Cjf"
      },
      "source": [
        "! nvidia-smi\n",
        "!/usr/local/cuda/bin/nvcc --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk7mjfbqYvdS"
      },
      "source": [
        "## Installation\r\n",
        "1. Install PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CJ6c5Y4psPl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "662d5f2e-8f27-4260-a7cf-2123d7070034"
      },
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.6.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruJju8S9r7o8"
      },
      "source": [
        "! pip install torch==1.4.0 torchvision==0.5.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjqXYyuhIJH7"
      },
      "source": [
        "2. Install [COCOAPI](https://github.com/cocodataset/cocoapi):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wroZnDoPIOcp"
      },
      "source": [
        "! pip install cython; pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ucy9RTJIZ0V"
      },
      "source": [
        "3. Clone this repo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NF_X5OgX2Qvz"
      },
      "source": [
        "CenterTrack_ROOT=\"CenterTrack\"\n",
        "! git clone --recursive https://github.com/xingyizhou/CenterTrack $CenterTrack_ROOT\n",
        "%cd CenterTrack"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvnAkG0jJgFZ"
      },
      "source": [
        "You can manually install the [submodules](https://github.com/xingyizhou/CenterTrack/blob/master/.gitmodules) if you forget --recursive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQJPHZVQIib3"
      },
      "source": [
        "4. Install the requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFIuWq-x2VrZ"
      },
      "source": [
        "! pip install -r requirements.txt\r\n",
        "! apt-get install libx264-dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKJ6tH2P23jN"
      },
      "source": [
        "%ls src/lib/model/networks/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDiYoRH7oM5x"
      },
      "source": [
        "#%rm -r src/lib/model/networks/DCNv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7-8pXOBJu8G"
      },
      "source": [
        "5. Compile deformable convolutional (from DCNv2)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCylGnMy2Xkj"
      },
      "source": [
        "# clone if it is not automatically downloaded by `--recursive`.\n",
        "%%shell\n",
        "\n",
        "cd src/lib/model/networks/\n",
        "git clone https://github.com/CharlesShang/DCNv2/  \n",
        "cd DCNv2/\n",
        "./make.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3t1BrZh-KAJ1"
      },
      "source": [
        "6. Download pertained models for [monocular 3D tracking](https://drive.google.com/open?id=1e8zR1m1QMJne-Tjp-2iY_o81hn2CiQRt), [80-category tracking](https://drive.google.com/open?id=1tJCEJmdtYIh8VuN8CClGNws3YO7QGd40), or [pose tracking](https://drive.google.com/open?id=1tJCEJmdtYIh8VuN8CClGNws3YO7QGd40) and move them to \n",
        "`$CenterTrack_ROOT/models/`. \n",
        "\n",
        "  More models can be found in [Model zoo](https://github.com/xingyizhou/CenterTrack/blob/master/readme/MODEL_ZOO.md)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5mlF2KXZddE"
      },
      "source": [
        "%mkdir models\r\n",
        "#! gdown --id 1e8zR1m1QMJne-Tjp-2iY_o81hn2CiQRt -O models/nuScenes_3Dtracking.pth\r\n",
        "#! gdown --id 1tJCEJmdtYIh8VuN8CClGNws3YO7QGd40 -O models/coco_tracking.pth\r\n",
        "#! gdown --id 1H0YvFYCOIZ06EzAkC2NxECNQGXxK27hH -O models/coco_pose_tracking.pth\r\n",
        "! gdown --id 1sf1bWJ1LutwQ_wp176nd2Y3HII9WeFf0 -O models/mot17_half.pth\r\n",
        "! gdown --id 1h_8Ts11rf0GQ4_n6FgmCeBuFcWrRjJfa -O models/mot17_fulltrain.pth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdHblfZC7wpX"
      },
      "source": [
        "#!pip install youtube-dl\n",
        "from IPython.display import YouTubeVideo\n",
        "\n",
        "YOUTUBE_ID = 'MNn9qKG2UFI'   # car\n",
        "#YOUTUBE_ID = 'MNn9qKG2UFI'   \n",
        "#YOUTUBE_ID = '2bKXv_XviFc'    # pedestrian \n",
        "\n",
        "YouTubeVideo(YOUTUBE_ID)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AF9KFFql7xkX"
      },
      "source": [
        "#! rm -rf youtube.mp4\n",
        "# download the youtube with the given ID\n",
        "! youtube-dl -f 'bestvideo[ext=mp4]' --output \"youtube2.%(ext)s\" https://www.youtube.com/watch?v=$YOUTUBE_ID\n",
        "# cut the first 5 seconds\n",
        "#! ffmpeg -y -loglevel info -i youtube2.mp4 -t 20 pedestrian.avi\n",
        "! ffmpeg -y -loglevel info -i youtube.mp4 -t 20 road_traffic.mp4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn0cPU8RfXbo"
      },
      "source": [
        "## Use CenterTrack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRsTsPCd-YAO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3d64c6d-aecf-42fb-c684-171f99e81f62"
      },
      "source": [
        "%cd src/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/CenterTrack/src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkGUyAR5NPyA"
      },
      "source": [
        "**Note:**\n",
        "1. replace \"cv2.imshow\" in function show_all_image in \"debugger.py(/src/lib/utils/)\" with \"pass\"\n",
        "2. comment out  \"cv2.imshow\" at line 80 in \"demo.py\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_eepSVd8VIa"
      },
      "source": [
        "Video clip from the [nuScenes dataset](https://www.nuscenes.org/?externalData=all&mapData=all&modalities=Any) in `videos/nuscenes_mini.mp4`. To test monocular 3D tracking on this video, run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzdHGbh-8DPi"
      },
      "source": [
        "! python demo.py tracking,ddd --load_model ../models/nuScenes_3Dtracking.pth --dataset nuscenes --pre_hm --track_thresh 0.1 --demo ../videos/nuscenes_mini.mp4 --test_focal_length 633 --save_video --video_h 480 --video_w 720 --debug 2\n",
        "#! python demo.py tracking,ddd --load_model ../models/nuScenes_3Dtracking.pth --dataset nuscenes --pre_hm --track_thresh 0.1 --demo road_traffic.avi --save_video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vZilmH_88bH"
      },
      "source": [
        "You will need to specify test_focal_length for monocular 3D tracking demo to convert the image coordinate system back to 3D. The value 633 is half of a typical focal length (~1266) in nuScenes dataset in input resolution 1600x900. The mini demo video is in an input resolution of 800x448, so we need to use a half focal length. You don't need to set the test_focal_length when testing on the original nuScenes data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra4B0CzB9XEx"
      },
      "source": [
        "Similarly, for 80-category tracking on images/ video, run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7B_4SKd1_n5"
      },
      "source": [
        "! python demo.py tracking --load_model ../models/coco_tracking.pth --demo road_traffic.mp4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfs1XUiW9fcn"
      },
      "source": [
        "If you want to test with person tracking models, you need to add --num_class 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5KPZwaT9gGR"
      },
      "source": [
        "! python demo.py tracking --load_model ../models/mot17_half.pth --num_class 1 --demo pedestrian.avi --save_video\n",
        "#! python demo.py tracking --load_model ../models/mot17_fulltrain.pth --num_class 1 --demo pedestrian.avi --save_video --debug 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSMzMJXC96IP"
      },
      "source": [
        "You can add --debug 2 to visualize the heatmap and offset predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAwdEdFT94W5"
      },
      "source": [
        "! python demo.py tracking --load_model ../models/mot17_half.pth --num_class 1 --debug 2 --demo video.mp4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TF8AOxtQ9lTy"
      },
      "source": [
        "For monocular 3D tracking, run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vnmvi9yL9nvo"
      },
      "source": [
        "! python demo.py tracking,ddd --demo ../road_traffic.mp4 --load_model ../models/coco_tracking.pth --save_video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6r6ZlYs9ud-"
      },
      "source": [
        "Similarly, for pose tracking, run:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyzdY5ju9u7k"
      },
      "source": [
        "! python demo.py tracking,multi_pose --load_model ../models/coco_pose.pth --demo video.mp4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjqq06mMem3Q"
      },
      "source": [
        "## Training on custom dataset\r\n",
        "If you want to train CenterTrack on your own dataset, you can use `--dataset custom` and manually specify the annotation file, image path, input resolutions, and number of categories. You still need to create the annotation files in COCO format (referring to the many `convert_X_to_coco.py` examples in tools). For example, you can use the following command to train on our [mot17 experiment](https://github.com/xingyizhou/CenterTrack/blob/master/experiments/mot17_half_sc.sh) without using the pre-defined mot dataset file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89uylqque2lG"
      },
      "source": [
        "! python main.py tracking --exp_id mot17_half_sc --dataset custom --custom_dataset_ann_path ../data/mot17/annotations/train_half.json --custom_dataset_img_path ../data/mot17/train/ --input_h 544 --input_w 960 --num_classes 1 --pre_hm --ltrb_amodal --same_aug --hm_disturb 0.05 --lost_disturb 0.4 --fp_disturb 0.1 --gpus 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u84xB-4YciO-"
      },
      "source": [
        "## Benchmark Evaluation and Training\r\n",
        "After installation, follow the instructions in [DATA.md](https://github.com/xingyizhou/CenterTrack/blob/master/readme/DATA.md) to setup the datasets. Then check [GETTING_STARTED.md](https://github.com/xingyizhou/CenterTrack/blob/master/readme/GETTING_STARTED.md) to reproduce the results in the paper. We provide scripts for all the experiments in the [experiments folder](https://github.com/xingyizhou/CenterTrack/blob/master/experiments)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uudcC2xMdO4t"
      },
      "source": [
        "### **Benchmark evaluation**\r\n",
        "First, download the models you want to evaluate from our model zoo and put them in CenterTrack_ROOT/models/.\r\n",
        "\r\n",
        "**MOT17**\r\n",
        "\r\n",
        "To test the tracking performance on MOT17 with our pretrained model, run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0SiynR5dRaM"
      },
      "source": [
        "! python test.py tracking --exp_id mot17_half --dataset mot --dataset_version 17halfval --pre_hm --ltrb_amodal --track_thresh 0.4 --pre_thresh 0.5 --load_model ../models/mot17_half.pth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Drou-llkdgiH"
      },
      "source": [
        "This will give a MOTA of `66.1` if set up correctly. `--pre_hm` is to enable the input heatmap. `--ltrb_amodal` is to use the left, top, right, bottom bounding box representation to enable detecting out-of-image bounding box (We observed this is important for MOT datasets). And `--track_thresh` and `--pre_thresh` are the score threshold for predicting a bounding box ($\\theta$ in the paper) and feeding the heatmap to the next frame ($\\tau$ in the paper), respectively.\r\n",
        "\r\n",
        "To test with public detection, run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFOAtnMzdhXT"
      },
      "source": [
        "! python test.py tracking --exp_id mot17_half_public --dataset mot --dataset_version 17halfval --pre_hm --ltrb_amodal --track_thresh 0.4 --pre_thresh 0.5 --load_model ../models/mot17_half.pth --public_det --load_results ../data/mot17/results/val_half_det.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzzCMMn3d4eo"
      },
      "source": [
        "The expected MOTA is `63.1`.\r\n",
        "\r\n",
        "To test on the test set, run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0J273JgeNVw"
      },
      "source": [
        "### Training\r\n",
        "We have packed all the training scripts in the [experiments folder](https://github.com/xingyizhou/CenterTrack/blob/master/experiments). The experiment names correspond to the model name in the [model zoo](https://github.com/xingyizhou/CenterTrack/blob/master/experiments). The number of GPUs for each experiment can be found in the scripts and the model zoo. If the training is terminated before finishing, you can use the same command with `--resume` to resume training. It will found the latest model with the same `exp_id`. Some experiments rely on pretraining on another model. In this case, download the pretrained model from our model zoo or train that model first."
      ]
    }
  ]
}